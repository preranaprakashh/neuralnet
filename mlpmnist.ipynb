{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJ9JQSW72Y0NiaFJENBr1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preranaprakashh/neuralnet/blob/main/mlpmnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "ZUif9dfCsGQ9",
        "outputId": "c3bb9db3-5ff0-482d-8a7a-ec06a8f1efcd"
      },
      "source": [
        "# loading the mnist dataset\n",
        "import tensorflow.keras.datasets.mnist as mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# number of elements in training and testing datasets\n",
        "m_train = x_train.shape[0]\n",
        "m_test = x_test.shape[0]\n",
        "image_size = x_train.shape[1]\n",
        "\n",
        "# summarize loaded dataset\n",
        "print('X_train: ' + str(x_train.shape))\n",
        "print('Y_train: ' + str(y_train.shape))\n",
        "print('X_test:  ' + str(x_test.shape))\n",
        "print('Y_test:  ' + str(y_test.shape[0]))\n",
        "print('Training images:', m_train)\n",
        "print('Test images:', m_test)\n",
        "print('height = width =', image_size)\n",
        "\n",
        "\n",
        "for i in range(9):\n",
        "    # define subplot\n",
        "    # plt.subplot(330 + 1 + i)\n",
        "    # # plot raw pixel data\n",
        "    # plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n",
        "    print(\"y = \" + str(y_train[i]))\n",
        "# show the figure\n",
        "plt.show()\n",
        "\n",
        "x_train_flat = x_train.reshape(x_train.shape[1]*x_train.shape[2], x_train.shape[0])\n",
        "x_test_flat = x_test.reshape(x_test.shape[1]*x_test.shape[2], x_test.shape[0])\n",
        "y_train = y_train.reshape(1, y_train.shape[0])\n",
        "y_test = y_test.reshape(1, y_test.shape[0])\n",
        "\n",
        "print (\"x_train_flat: \" + str(x_train_flat.shape))\n",
        "print (\"x_test_flat: \" + str(x_test_flat.shape))\n",
        "print (\"y_train:\", y_train.shape, \"y_test\", y_test.shape)\n",
        "\n",
        "x_train = x_train_flat/255\n",
        "x_test = x_test_flat/255\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "print(\"changeeeeeeeee\", y_train.shape)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    x_exp = np.exp(x)\n",
        "    x_sum = np.sum(x_exp, axis=1, keepdims = True)\n",
        "    Z = x_exp/x_sum \n",
        "    cache = x\n",
        "    assert(Z.shape == x.shape)\n",
        "    return Z, x\n",
        "\n",
        "\n",
        "def softmax_backward(x, cache):\n",
        "    S = cache\n",
        "    print(\"size of softmax = \", S.shape)\n",
        "    S_vector = S.reshape(S.shape[0],1)\n",
        "    S_matrix = np.tile(S_vector,S.shape[0])\n",
        "    der = np.diag(S) - (S_matrix * S_matrix.T)\n",
        "    assert (der.shape == x.shape)\n",
        "    return der\n",
        "\n",
        "#initialize parameters to random numbers for W and zeros for b\n",
        "def parameter_init(layer_num):\n",
        "    parameters = {}\n",
        "    L = len(layer_num)\n",
        "    for l in range(1, L):\n",
        "        parameters['W'+str(l)] = np.random.randn(layer_num[l], layer_num[l-1])\n",
        "        parameters['b'+ str(l)] = np.zeros((layer_num[l], 1))\n",
        "    return parameters\n",
        "\n",
        "\n",
        "def fwd_lin(A, W, b):\n",
        "    # to calculate Z value for all layers\n",
        "    Z = np.dot(W, A) + b #Z[L] = W[L]A[L-1] + b[L]\n",
        "    lin_cache = (A, W, b) #inputs A, W and b are stored in cache\n",
        "    \n",
        "    return Z, lin_cache\n",
        "\n",
        "def fwd_acti(Aprev, W, b, acti):\n",
        "    if acti == \"relu\":\n",
        "        Z, lin_cache = fwd_lin(Aprev, W, b) #calculate Z for each layer\n",
        "        A, acti_cache = relu(Z) #use relu activation function \n",
        "    elif acti == \"softmax\":\n",
        "        Z, lin_cache = fwd_lin(Aprev, W, b)\n",
        "        A, acti_cache = softmax(Z)\n",
        "    cache = (lin_cache, acti_cache)\n",
        "    return A, cache\n",
        "\n",
        "def fwd(X, parameters):\n",
        "    #to define the activation function for every layer and implement it\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # number of layers\n",
        "    for l in range(1, L):\n",
        "        Aprev = A \n",
        "        A, cache = fwd_acti(Aprev, parameters['W' + str(l)], parameters['b' + str(l)], acti = \"relu\") #for hidden layers\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = fwd_acti(A, parameters['W' + str(L)], parameters['b' + str(L)], acti = \"softmax\") #for output layer\n",
        "    caches.append(cache)\n",
        "   \n",
        "    return AL, caches\n",
        "\n",
        "def costx(AL, Y):\n",
        "    m = Y.shape[1]\n",
        "    print(Y.shape)\n",
        "    print(AL.shape)\n",
        "    cost = -1/m * np.sum(np.multiply(Y, np.log(AL))+np.multiply((1-Y), np.log(1-AL)), axis = 1, keepdims = True) #J = sum(yloga+(1-y)log(1-a))\n",
        "    cost = np.squeeze(cost)    \n",
        "    return cost\n",
        "\n",
        "def back_lin(dZ, cache):\n",
        "    Aprev, W, b = cache\n",
        "    m = Aprev.shape[1]\n",
        "    dW = 1/m * np.dot(dZ, Aprev.T)\n",
        "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def back_acti(dA, cache, acti):\n",
        "    lin_cache, acti_cache = cache\n",
        "    \n",
        "    if acti == \"relu\":\n",
        "        dZ = relu_backward(dA, acti_cache)\n",
        "        dA_prev, dW, db = back_lin(dZ, lin_cache)\n",
        "        \n",
        "    else: #if acti == \"softmax\"\n",
        "        dZ = softmax_backward(dA, acti_cache)\n",
        "        dA_prev, dW, db = back_lin(dZ, lin_cache)\n",
        "        \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def bwd(AL, Y, caches):\n",
        "    grads = {} # the gradients are stored in grads\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
        "    \n",
        "    # last layer is softmax\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp, dW_temp, db_temp = back_acti(dAL, current_cache, acti = \"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "    grads[\"db\" + str(L)] = db_temp\n",
        "\n",
        "    # hidden layers are relu\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = back_acti(grads[\"dA\" + str(l+1)], current_cache, acti = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l+1)] = dW_temp\n",
        "        grads[\"db\" + str(l+1)] = db_temp\n",
        "    \n",
        "    return grads\n",
        "\n",
        "def parameter_update(params, grads, learning_rate):\n",
        "    parameters = params.copy()\n",
        "    L = len(parameters) // 2 # number of layers\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "\n",
        "\n",
        "layer_dim = [784, 300, 100, 10] #  3-layer model\n",
        "\n",
        "def three_layer_model(X, Y, layer_dim, learning_rate = 0.0075, iterations = 3000, print_cost=False):\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    parameters = parameter_init(layer_dim)\n",
        "    \n",
        "    for i in range(0, iterations):\n",
        "        AL, caches = fwd(X, parameters) #forward propagation\n",
        "        cost = costx(AL, Y)#compute cost\n",
        "        grads = bwd(AL, Y, caches)#backward propagation\n",
        "        parameters = parameter_update(parameters, grads, learning_rate)#update parameters\n",
        "\n",
        "        if print_cost and i % 100 == 0 or i == iterations - 1:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if i % 100 == 0 or i == iterations:\n",
        "            costs.append(cost)\n",
        "    \n",
        "    return parameters, costs\n",
        "\n",
        "parameters, costs = three_layer_model(x_train, y_train, layer_dim, iterations = 2500, print_cost = True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  10000\n",
            "Training images: 60000\n",
            "Test images: 10000\n",
            "height = width = 28\n",
            "y = 5\n",
            "y = 0\n",
            "y = 4\n",
            "y = 1\n",
            "y = 9\n",
            "y = 2\n",
            "y = 1\n",
            "y = 3\n",
            "y = 1\n",
            "x_train_flat: (784, 60000)\n",
            "x_test_flat: (784, 10000)\n",
            "y_train: (1, 60000) y_test (1, 10000)\n",
            "changeeeeeeeee (1, 60000, 10)\n",
            "(1, 60000, 10)\n",
            "(10, 60000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:138: RuntimeWarning: divide by zero encountered in log\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ab1451ee363d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthree_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ab1451ee363d>\u001b[0m in \u001b[0;36mthree_layer_model\u001b[0;34m(X, Y, layer_dim, learning_rate, iterations, print_cost)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#forward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcostx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#compute cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#backward propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-ab1451ee363d>\u001b[0m in \u001b[0;36mcostx\u001b[0;34m(AL, Y)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#J = sum(yloga+(1-y)log(1-a))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,60000,10) (10,60000) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOYR8myyzIrn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}